{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Streaming from Kafka"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Configure magic</b> - this imports the correct java class versions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 953.56201171875,
      "end_time": 1649625861034.895
     }
    }
   },
   "outputs": [],
   "source": [
    "%%configure -f\n",
    "{\n",
    "    \"conf\": { \n",
    "        \"spark.jars.packages\": \"org.apache.spark:spark-sql-kafka-0-10_2.11:2.2.0\"\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following command fires up the spark session and context (if not already started) and returns the version number.<br/> \n",
    "Use this info to check 2.2.0 version number in the above package name is correct. This only needs to be run if you experience errors in the above magic execution "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 90.047119140625,
      "end_time": 1649625762518.106
     }
    }
   },
   "outputs": [],
   "source": [
    "sc.version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do some imports<br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 275.56591796875,
      "end_time": 1649625927599.66
     }
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import StructType,StructField, StringType, IntegerType, DoubleType, TimestampType, LongType\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensures only one copy of the session object<br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 529.5859375,
      "end_time": 1649625936154.396
     }
    }
   },
   "outputs": [],
   "source": [
    "def getSparkSessionInstance():\n",
    "    if ('sparkSessionSingletonInstance' not in globals()):\n",
    "        globals()['sparkSessionSingletonInstance'] = SparkSession\\\n",
    "            .builder\\\n",
    "            .appName(\"Structured Streaming \") \\\n",
    "            .master(\"local[*]\") \\\n",
    "            .getOrCreate()\n",
    "    return globals()['sparkSessionSingletonInstance']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Starts a session (if you haven't already or if you've stopped the old one)<br/>\n",
    "Creates a new context<br/>\n",
    "Sets the error level to prevent INFO and WARNING line showing<br/>\n",
    "Creates the streaming context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 5876.495849609375,
      "end_time": 1649625947029.24
     }
    }
   },
   "outputs": [],
   "source": [
    "spark = getSparkSessionInstance()\n",
    "sc = spark.sparkContext\n",
    "sc.setLogLevel(\"ERROR\")\n",
    "ssc = StreamingContext(sc, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Assign Values</b> - Overwite the kafkaBrokers value with your \"main\" Azure host"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 87.453857421875,
      "end_time": 1649625956769.375
     }
    }
   },
   "outputs": [],
   "source": [
    "kafkaBrokers = \"192.168.0.10:9092\"\n",
    "kafkaTopic = \"SensorReadings\"\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This sets up the input stream to your Azure cluster and reads off the event stream.<br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 818.93505859375,
      "end_time": 1649625962954.461
     }
    }
   },
   "outputs": [],
   "source": [
    "df = spark \\\n",
    "    .readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", kafkaBrokers) \\\n",
    "    .option(\"subscribe\", kafkaTopic) \\\n",
    "    .option(\"startingOffsets\", \"earliest\") \\\n",
    "    .load()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a Kafka stream, both the key and value are byte arrays. We cast them as strings.<br/> \n",
    "The timestamp is a long integer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 819.428955078125,
      "end_time": 1649625971743.728
     }
    }
   },
   "outputs": [],
   "source": [
    "df1 = df.selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\", \"timestamp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our sensor data is structured as a JSON string within the value field. The structure is defined in the schema below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 79.3798828125,
      "end_time": 1649625979773.237
     }
    }
   },
   "outputs": [],
   "source": [
    "value_schema = StructType([\n",
    "        StructField(\"sensor\",StringType(),True),\n",
    "        StructField(\"machine\",StringType(),True),\n",
    "        StructField(\"units\",StringType(),True),\n",
    "        StructField(\"time\", LongType(), True),\n",
    "        StructField(\"value\", DoubleType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we extract the data using the schema above.<br/>\n",
    "We also cast the kafka-style timestamp to a unix-style timestamp. This is needed for the windowed streaming later.<br/>\n",
    "We create a temporary view over the dataframe to use the following SQL command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 1348.4130859375,
      "end_time": 1649625987824.559
     }
    }
   },
   "outputs": [],
   "source": [
    "df2 = df1.withColumn(\"jsonData\", from_json(col(\"value\"), value_schema)) \\\n",
    "        .withColumn(\"timestamp\", (col(\"timestamp\").cast(\"long\")/1000).cast(\"timestamp\")) \\\n",
    "        .select(\"key\", \"jsonData.*\", \"timestamp\")\n",
    "\n",
    "df2.createOrReplaceTempView(\"sensor_find\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This SQL creates a data frame showing the key (sensor name) and the units.<br/>\n",
    "This simply give the values more meaning in the scenario output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 427.634033203125,
      "end_time": 1649625993715.051
     }
    }
   },
   "outputs": [],
   "source": [
    "streamingDataFrame = spark.sql('SELECT timestamp, concat(key, \" (\", units, \")\") AS key, value FROM sensor_find')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two options for output are given:<br/>\n",
    "<b>Grouped</b> - Shows the average value per sensor. This option just lists the sensor and average value over all readings.<br>\n",
    "<b>Windowed</b> - This is probably a more realistic output in a similar real-life scenario but (depending on window values) displays a long list.<br/>\n",
    "For displaying in a notebook, the shorter list is preferable "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 391.14501953125,
      "end_time": 1649625998599.451
     }
    }
   },
   "outputs": [],
   "source": [
    "outStream = streamingDataFrame. \\\n",
    "    groupBy(\"key\"). \\\n",
    "    avg('value')\n",
    "\n",
    "#outStream = streamingDataFrame. \\\n",
    "#    withWatermark(\"timestamp\", \"1 minute\"). \\\n",
    "#    groupBy(window(\"timestamp\", \"10 minutes\", \"5 minutes\"), \"key\"). \\\n",
    "#    groupBy(\"key\"). \\\n",
    "#    avg('value')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The aggregation process creates a new aggreagated column. We rename it for readability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 299.756103515625,
      "end_time": 1649626005325.478
     }
    }
   },
   "outputs": [],
   "source": [
    "outStream = outStream.withColumnRenamed(\"avg(value)\", \"value\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell gathers a batch of records and streams them to the console then wait for the next batch to arrive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = outStream.writeStream. \\\n",
    "    outputMode('complete'). \\\n",
    "    option(\"numRows\", 1000). \\\n",
    "    option(\"truncate\", \"false\"). \\\n",
    "    format(\"console\"). \\\n",
    "    start(). \\\n",
    "    awaitTermination()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark3",
   "language": "",
   "name": "pyspark3kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "mimetype": "text/x-python",
   "name": "pyspark3",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
