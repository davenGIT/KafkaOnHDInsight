{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Streaming from Kafka"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This command fires up the spark session and context and returns the version number.<br/> \n",
    "Use this info to update the configuration below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Configure magic</b> - this imports the correct java class versions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%configure -f\n",
    "{\n",
    "    \"Conf\":\t{\"spark.jars.packages\":\"org.apache.spark:spark-sql-kafka-0-10_2.11:2.4.8\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do some imports<br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import StructType,StructField, StringType, IntegerType, DoubleType, TimestampType, LongType\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensures only one copy of the session object<br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSparkSessionInstance():\n",
    "    if ('sparkSessionSingletonInstance' not in globals()):\n",
    "        globals()['sparkSessionSingletonInstance'] = SparkSession\\\n",
    "            .builder\\\n",
    "            .appName(\"Structured Streaming \") \\\n",
    "            .master(\"local[*]\") \\\n",
    "            .getOrCreate()\n",
    "    return globals()['sparkSessionSingletonInstance']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Starts a session (if you haven't already or if you've stopped the old one)<br/>\n",
    "Creates a new context<br/>\n",
    "Sets the error level to prevent INFO and WARNING line showing<br/>\n",
    "Creates the streaming context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = getSparkSessionInstance()\n",
    "sc = spark.sparkContext\n",
    "sc.setLogLevel(\"ERROR\")\n",
    "ssc = StreamingContext(sc, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Assign Values</b> - Overwite the kafkaBrokers value with your Azure hosts "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "kafkaBrokers = ['localhost:9092','otherhost:9092']\n",
    "kafkaTopic = \"SensorReadings\"\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This sets up the input stream to your Azure cluster and reads off the event stream.<br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = spark \\\n",
    "    .readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", kafkaBrokers)\n",
    "    .option(\"subscribe\", kafkaTopic)\n",
    "    .option(\"startingOffsets\", \"earliest\")\n",
    "    .load()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a Kafka stream, both the key and value are byte arrays. We cast them as strings.<br/> \n",
    "The timestamp is a long integer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df1 = df.selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\", \"timestamp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our sensor data is structured as a JSON string within the value field. The structure is defined in the schema below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "value_schema = StructType([\n",
    "        StructField(\"sensor\",StringType(),True),\n",
    "        StructField(\"machine\",StringType(),True),\n",
    "        StructField(\"units\",StringType(),True),\n",
    "        StructField(\"time\", LongType(), True),\n",
    "        StructField(\"value\", DoubleType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we extract the data using the schema above.<br/>\n",
    "We also cast the kafka-style timestamp to a unix-style timestamp. This is only necessary if using the windowed streaming later.<br/>\n",
    "We create a temporary view over the dataframe to use the following SQL command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df2 = df1.withColumn(\"jsonData\", from_json(col(\"value\"), value_schema)) \\\n",
    "        .withColumn(\"timestamp\", (col(\"timestamp\")/1000).cast(\"timestamp\")) \\\n",
    "        .select(\"key\", \"jsonData.*\", \"timestamp\")\n",
    "\n",
    "df2.createOrReplaceTempView(\"sensor_find\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This SQL creates a data frame showing the key (sensor name) and the units.<br/>\n",
    "This simply give the values more meaning in the scenario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "streamingDataFrame = spark.sql('SELECT timestamp, concat(key, \" (\", units, \")\") AS key, value FROM sensor_find')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two options for output are given:<br/>\n",
    "<b>Grouped</b> - Shows the average value per sensor. This option just lists the sensor and average value over all readings.<br>\n",
    "<b>Windowed</b> - This is probably a more realistic output in a similar real-life scenario but (depending on window values) displays a long list.<br/>\n",
    "For displaying in a notebook, the shorter list is preferable "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outStream = streamingDataFrame. \\\n",
    "    groupBy(\"key\"). \\\n",
    "    avg('value')\n",
    "\n",
    "#outStream = streamingDataFrame. \\\n",
    "#    withWatermark(\"timestamp\", \"1 minute\"). \\\n",
    "#    groupBy(window(\"timestamp\", \"10 minutes\", \"5 minutes\"), \"key\"). \\\n",
    "#    groupBy(\"key\"). \\\n",
    "#    avg('value')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The aggregation process creates a new aggreagated column. We rename it for readability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outStream = outStream.withColumnRenamed(\"avg(value)\", \"value\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell gathers a batch of records and streams them to the console then wait for the next batch to arrive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = outStream.writeStream. \\\n",
    "    outputMode('complete'). \\\n",
    "    option(\"numRows\", 1000). \\\n",
    "    option(\"truncate\", \"false\"). \\\n",
    "    format(\"console\"). \\\n",
    "    start(). \\\n",
    "    awaitTermination()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark3",
   "language": "",
   "name": "pyspark3kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 2
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
